diff --git a/arch/x86/include/asm/spinlock.h b/arch/x86/include/asm/spinlock.h
index 64b6117..0964cfd 100644
--- a/arch/x86/include/asm/spinlock.h
+++ b/arch/x86/include/asm/spinlock.h
@@ -62,7 +62,17 @@ static inline void __ticket_unlock_kick(arch_spinlock_t *lock,
 #endif /* CONFIG_PARAVIRT_SPINLOCKS */
 static inline int  __tickets_equal(__ticket_t one, __ticket_t two)
 {
-	return !((one ^ two) & ~TICKET_SLOWPATH_FLAG);
+	return !((one ^ two) & ~LOCK_HOLD_FLAG);
+}
+
+extern u64 pmtlock_timeout_shift;
+static inline u64 __timeout_threshold(__ticket_t head, __ticket_t tail)
+{
+	// can detect LHP or LWP here by check LOCK_HOLD_FLAG on head,
+	// optimization??  
+	// the (__ticket_t) trick handles tail wraparound case
+	// e.g. (unsigned char)(1 - 255) = 2
+	return (__ticket_t)(tail - head) << pmtlock_timeout_shift;
 }
 
 static inline void __ticket_check_and_clear_slowpath(arch_spinlock_t *lock,
@@ -102,24 +112,40 @@ static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)
 static __always_inline void arch_spin_lock(arch_spinlock_t *lock)
 {
 	register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };
+ 	u64 timeout = 0;
+ 	__ticket_t current_head;
 
 	inc = xadd(&lock->tickets, inc);
-	if (likely(inc.head == inc.tail))
-		goto out;
+	if (likely(__tickets_equal(inc.head, inc.tail)))
+		goto spin;
+
+
+	timeout = __timeout_threshold(inc.head, inc.tail);
+	do {
+		current_head = ACCESS_ONCE(lock->tickets.head);
+		if (__tickets_equal(current_head, inc.tail)) {
+			goto spin;
+		} else if (!__tickets_equal(current_head, inc.head) && ((s16)(inc.tail - current_head) > 0)) {
+			// dynamic timout update when ticket head is updated. 
+			// This is complication, may not be necessary
+			// (s16)(tail - head) > 0 to filtered out waked up lock holder
+			inc.head = current_head;
+			timeout = __timeout_threshold(inc.head, inc.tail);
+		}
+		cpu_relax();
+	} while (timeout--);
 
+spin:
 	for (;;) {
-		unsigned count = SPIN_THRESHOLD;
-
-		do {
-			inc.head = READ_ONCE(lock->tickets.head);
-			if (__tickets_equal(inc.head, inc.tail))
-				goto clear_slowpath;
-			cpu_relax();
-		} while (--count);
-		__ticket_lock_spinning(lock, inc.tail);
-	}
-clear_slowpath:
-	__ticket_check_and_clear_slowpath(lock, inc.head);
+		current_head = ACCESS_ONCE(lock->tickets.head);
+		if (cmpxchg(&lock->tickets.head, 
+				current_head & ~LOCK_HOLD_FLAG, 
+				current_head | LOCK_HOLD_FLAG)
+			== (current_head & ~LOCK_HOLD_FLAG))
+			goto out;
+		cpu_relax();
+ 	}
+
 out:
 	barrier();	/* make sure nothing creeps before the lock is taken */
 }
@@ -131,9 +157,13 @@ static __always_inline int arch_spin_trylock(arch_spinlock_t *lock)
 	old.tickets = READ_ONCE(lock->tickets);
 	if (!__tickets_equal(old.tickets.head, old.tickets.tail))
 		return 0;
+	// not necessary since it's impossible for head==tail && hold_flag==1
+	//if ((old.tickets.head | LOCK_HOLD_FLAG) != 0)
+	//	return 0;
+
 
 	new.head_tail = old.head_tail + (TICKET_LOCK_INC << TICKET_SHIFT);
-	new.head_tail &= ~TICKET_SLOWPATH_FLAG;
+	new.tickets.head |= LOCK_HOLD_FLAG; 
 
 	/* cmpxchg is a full barrier, so nothing can move before it */
 	return cmpxchg(&lock->head_tail, old.head_tail, new.head_tail) == old.head_tail;
@@ -141,6 +171,7 @@ static __always_inline int arch_spin_trylock(arch_spinlock_t *lock)
 
 static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)
 {
+/*
 	if (TICKET_SLOWPATH_FLAG &&
 		static_key_false(&paravirt_ticketlocks_enabled)) {
 		__ticket_t head;
@@ -154,21 +185,23 @@ static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)
 			__ticket_unlock_kick(lock, (head + TICKET_LOCK_INC));
 		}
 	} else
-		__add(&lock->tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);
+*/
+	// xadd 1 works because +2 equals set LOCK_HOLD_FLAG and plus 1
+	__add(&lock->tickets.head, 1, UNLOCK_LOCK_PREFIX);
 }
 
 static inline int arch_spin_is_locked(arch_spinlock_t *lock)
 {
 	struct __raw_tickets tmp = READ_ONCE(lock->tickets);
 
-	return !__tickets_equal(tmp.tail, tmp.head);
+	return !__tickets_equal(tmp.tail, tmp.head) || ((tmp.head | LOCK_HOLD_FLAG) != 0);
 }
 
 static inline int arch_spin_is_contended(arch_spinlock_t *lock)
 {
 	struct __raw_tickets tmp = READ_ONCE(lock->tickets);
 
-	tmp.head &= ~TICKET_SLOWPATH_FLAG;
+	tmp.head &= ~LOCK_HOLD_FLAG;
 	return (__ticket_t)(tmp.tail - tmp.head) > TICKET_LOCK_INC;
 }
 #define arch_spin_is_contended	arch_spin_is_contended
diff --git a/arch/x86/include/asm/spinlock_types.h b/arch/x86/include/asm/spinlock_types.h
index 5f9d757..d912c94 100644
--- a/arch/x86/include/asm/spinlock_types.h
+++ b/arch/x86/include/asm/spinlock_types.h
@@ -7,10 +7,12 @@
 #define __TICKET_LOCK_INC	2
 #define TICKET_SLOWPATH_FLAG	((__ticket_t)1)
 #else
-#define __TICKET_LOCK_INC	1
+#define __TICKET_LOCK_INC	2
 #define TICKET_SLOWPATH_FLAG	((__ticket_t)0)
+#define LOCK_HOLD_FLAG		((__ticket_t)1)
 #endif
 
+/*
 #if (CONFIG_NR_CPUS < (256 / __TICKET_LOCK_INC))
 typedef u8  __ticket_t;
 typedef u16 __ticketpair_t;
@@ -18,6 +20,11 @@ typedef u16 __ticketpair_t;
 typedef u16 __ticket_t;
 typedef u32 __ticketpair_t;
 #endif
+*/
+
+// fix lock size here to handle ticket number wraparound
+typedef u16 __ticket_t;
+typedef u32 __ticketpair_t;
 
 #define TICKET_LOCK_INC	((__ticket_t)__TICKET_LOCK_INC)
 
diff --git a/kernel/locking/spinlock.c b/kernel/locking/spinlock.c
index db3ccb1..de4fa6c 100644
--- a/kernel/locking/spinlock.c
+++ b/kernel/locking/spinlock.c
@@ -21,6 +21,11 @@
 #include <linux/debug_locks.h>
 #include <linux/export.h>
 
+// timeout threshold for waiter N = N * 2 ^ pmtlock_timeout_shift
+// the larger the shift is, the more fairer pmtlock is
+u64 pmtlock_timeout_shift = 14;
+EXPORT_SYMBOL(pmtlock_timeout_shift);
+
 /*
  * If lockdep is enabled then we use the non-preemption spin-ops
  * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
